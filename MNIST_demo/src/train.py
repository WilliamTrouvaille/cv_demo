import torch
import torch.nn as nn
import torchvision
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader

from src.utils import Loader, LoggerHandler, TrainingProgress


class CNN(nn.Module):
    """ç®€å•çš„CNNç½‘ç»œç»“æ„ï¼Œç”¨äºMNISTåˆ†ç±»"""

    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)


class MNISTTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = LoggerHandler()
        self._init_components()
        self.best_acc = 0.0
        self.early_stop_counter = 0
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    def _init_components(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½ã€æ¨¡å‹ã€ä¼˜åŒ–å™¨ç­‰ç»„ä»¶"""
        # æ•°æ®é¢„å¤„ç†
        transform = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])

        # æ•°æ®é›†åŠ è½½
        train_set = torchvision.datasets.MNIST(
            root=Loader.get_path("/data"),
            train=True,
            download=True,
            transform=transform
        )
        val_set = torchvision.datasets.MNIST(
            root=Loader.get_path("/data"),
            train=False,
            download=True,
            transform=transform
        )

        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_set,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=2
        )
        self.val_loader = DataLoader(
            val_set,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=2
        )

        # æ¨¡å‹åˆå§‹åŒ–
        self.model = CNN().to(self.device)
        self.optimizer = Adam(
            self.model.parameters(),
            lr=self.config['training']['learning_rate']
        )
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            patience=2,
            factor=0.5,
            verbose=False
        )

    def _save_model(self, epoch, is_best=False):
        """æ¨¡å‹ä¿å­˜é€»è¾‘"""
        model_path = Loader.build_path(self.config['paths']['model_save'])
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_acc': self.best_acc
        }, model_path)
        if is_best:
            self.logger.info(f"** æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}ï¼Œå‡†ç¡®ç‡ {self.best_acc:.4f} **")

    def _train_epoch(self, epoch, progress):
        """å•ä¸ªepochçš„è®­ç»ƒé€»è¾‘"""
        self.model.train()
        train_loss, correct, total = 0.0, 0, 0

        batch_progress = progress.batch_progress('train', epoch)
        for inputs, labels in self.train_loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            batch_progress.update(1)
            progress.update_metrics(batch_progress, {
                'loss': train_loss / (batch_progress.n + 1e-5),
                'acc': 100. * correct / total
            })

        batch_progress.close()
        epoch_loss = train_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total
        return epoch_loss, epoch_acc

    def _validate(self, epoch, progress):
        """éªŒè¯é˜¶æ®µ"""
        self.model.eval()
        val_loss, correct, total = 0.0, 0, 0

        batch_progress = progress.batch_progress('val', epoch)
        with torch.no_grad():
            for inputs, labels in self.val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

                batch_progress.update(1)
                progress.update_metrics(batch_progress, {
                    'loss': val_loss / (batch_progress.n + 1e-5),
                    'acc': 100. * correct / total
                })

        batch_progress.close()
        epoch_loss = val_loss / len(self.val_loader)
        epoch_acc = 100. * correct / total
        return epoch_loss, epoch_acc

    def train(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        total_epochs = self.config['training']['epochs']
        progress = TrainingProgress(
            total_epochs=total_epochs,
            train_loader_len=len(self.train_loader),
            val_loader_len=len(self.val_loader),
            metrics=['loss', 'acc']
        )

        self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        self.logger.info(f"è®¾å¤‡ç±»å‹: {self.device}")
        self.logger.info(f"è®­ç»ƒå‚æ•°: {self.config['training']}")

        for epoch in range(1, total_epochs + 1):
            epoch_progress = progress.epoch_progress(epoch)

            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(epoch, progress)
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)

            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate(epoch, progress)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)

            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step(val_acc)

            # ä¿å­˜æœ€æ–°æ¨¡å‹
            self._save_model(epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_acc:
                self.best_acc = val_acc
                self._save_model(epoch, is_best=True)
                self.early_stop_counter = 0
            else:
                self.early_stop_counter += 1

            # æ˜¾ç¤ºæœ€è¿‘5ä¸ªepochçš„æŒ‡æ ‡
            recent_epochs = self.history['val_acc'][-5:]
            epoch_progress.set_postfix({
                'best': f"{self.best_acc:.2f}%",
                'recent': f"{min(recent_epochs):.2f}~{max(recent_epochs):.2f}%"
            })
            epoch_progress.close()

            # æ—©åœæ£€æµ‹
            if self.config['training']['early_stopping']['enable'] and \
                    self.early_stop_counter >= self.config['training']['early_stopping']['patience']:
                self.logger.warning(f"âš ï¸ æ—©åœè§¦å‘ï¼šè¿ç»­{self.early_stop_counter}ä¸ªepochæœªæå‡éªŒè¯å‡†ç¡®ç‡")
                break

        self.logger.info("âœ… è®­ç»ƒå®Œæˆ")


def main():
    config = Loader.load_config()
    trainer = MNISTTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main()
